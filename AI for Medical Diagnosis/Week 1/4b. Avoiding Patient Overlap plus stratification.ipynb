{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2a685ee-a485-485f-9c91-a3f040135683",
   "metadata": {},
   "source": [
    "# Split into train, val and test with no patient id overlap\n",
    "\n",
    "# plus ensure that the same pct of 1s & 0s in each disease column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9591d3c1-ac54-452a-81ae-3880e239767d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of stratification key after re-aggregation:\n",
      "A stratify_key of 0 means this number of people had none of the 14 diseases\n",
      "stratify_key\n",
      " 0    582\n",
      " 1    262\n",
      " 2    112\n",
      " 3     50\n",
      "-1      8\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the datasets\n",
    "df1 = pd.read_csv(\"data/nih/train-small.csv\")  \n",
    "df2 = pd.read_csv(\"data/nih/valid-small.csv\")\n",
    "\n",
    "# Concatenate the two DataFrames along rows\n",
    "df = (pd.concat([df1, df2], axis=0, ignore_index=True)\n",
    "      .sample(frac=1, random_state=42)\n",
    "      .reset_index(drop=True))\n",
    "\n",
    "# Ensure 'PatientId' is correctly specified\n",
    "patient_df = df.groupby('PatientId').first().reset_index()  # Group by PatientId to get unique rows for each patient\n",
    "\n",
    "# Specify the columns that you want to use for stratification (columns with 1s and 0s)\n",
    "stratification_columns = list(df.columns)\n",
    "stratification_columns.remove('Image')\n",
    "stratification_columns.remove('PatientId')\n",
    "\n",
    "# Create a stratification key (sum of the selected columns or any other strategy)\n",
    "patient_df['stratify_key'] = patient_df[stratification_columns].sum(axis=1)\n",
    "\n",
    "# Group by stratification key and get counts\n",
    "stratify_counts = patient_df['stratify_key'].value_counts()\n",
    "\n",
    "# Combine rare classes to ensure a minimum of min_class_size samples per class\n",
    "min_class_size = 8\n",
    "patient_df['stratify_key'] = patient_df['stratify_key'].apply(\n",
    "    lambda x: x if stratify_counts[x] >= min_class_size else -1  # Use -1 or any other placeholder value\n",
    ")\n",
    "\n",
    "# Check the distribution of stratify_key after combining rare classes\n",
    "print(\"Distribution of stratification key after re-aggregation:\")\n",
    "print(\"A stratify_key of 0 means this number of people had none of the 14 diseases\")\n",
    "print(patient_df['stratify_key'].value_counts())\n",
    "\n",
    "# Verify that no class has fewer than min_class_size samples\n",
    "assert all(patient_df['stratify_key'].value_counts() >= 2), \"Some classes still have fewer than 2 samples!\"\n",
    "\n",
    "# Now perform the train-test split\n",
    "train_ids, temp_ids = train_test_split(patient_df['PatientId'], \n",
    "                                       test_size=0.2, \n",
    "                                       stratify=patient_df['stratify_key'], \n",
    "                                       random_state=42)\n",
    "\n",
    "# Create a stratification key for the remaining set\n",
    "temp_df = patient_df[patient_df['PatientId'].isin(temp_ids)]\n",
    "\n",
    "# Split temp_ids into validation and test sets\n",
    "val_ids, test_ids = train_test_split(temp_df['PatientId'], \n",
    "                                     test_size=0.5, \n",
    "                                     stratify=temp_df['stratify_key'], \n",
    "                                     random_state=42)\n",
    "\n",
    "# Create train, validation, and test sets from the original DataFrame based on patient IDs\n",
    "train_df = df[df['PatientId'].isin(train_ids)]\n",
    "val_df   = df[df['PatientId'].isin(val_ids)]\n",
    "test_df  = df[df['PatientId'].isin(test_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "489897cb-823c-42a4-9e81-670a7948e227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 895\n",
      "Validation set size: 106\n",
      "Test set size: 108\n"
     ]
    }
   ],
   "source": [
    "# Display the sizes of each set\n",
    "print(f\"Train set size: {train_df.shape[0]}\")\n",
    "print(f\"Validation set size: {val_df.shape[0]}\")\n",
    "print(f\"Test set size: {test_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e838fd-0849-40ee-b746-87498b6226cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mac_gpu] *",
   "language": "python",
   "name": "conda-env-mac_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
